Review of Course 4: Machine Learning, Visual Recognition, and NLP
Model Evaluation and Performance Metrics
Evaluation metrics
Supervised learning is the focus of this course, but it should be seen in the context of the other learning fields.

supervised learning

unsupervised learning

semi-supervised learning

reinforcement learning

As a reminder the type of 
supervised learning
 depends on the data type of the target. The supervised learning problem is referred to as either

Regression (when 
Y
YY is real-valued)

e.g., if you are predicting price, demand, or number of subscriptions.
 

or

Classification (when 
Y
YY is categorical)

e.g., if you are prediction fraud or churn.

Regression
The two following metrics are the most commonly used.

RMSE
=
1
N
∑
i
=
1
n
(
y
^
i
−
y
i
)
2
RMSE= 
N
1
​
 ∑ 
i=1
n
​
 ( 
y
^
​
  
i
​
 −y 
i
​
 ) 
2
 
​
 start text, R, M, S, E, end text, equals, square root of, start fraction, 1, divided by, N, end fraction, sum, start subscript, i, equals, 1, end subscript, start superscript, n, end superscript, left parenthesis, y, with, hat, on top, start subscript, i, end subscript, minus, y, start subscript, i, end subscript, right parenthesis, squared, end square root

MAE
=
1
N
∑
i
=
1
n
∣
y
^
i
−
y
i
∣
MAE= 
N
1
​
 ∑ 
i=1
n
​
 ∣ 
y
^
​
  
i
​
 −y 
i
​
 ∣start text, M, A, E, end text, equals, start fraction, 1, divided by, N, end fraction, sum, start subscript, i, equals, 1, end subscript, start superscript, n, end superscript, open vertical bar, y, with, hat, on top, start subscript, i, end subscript, minus, y, start subscript, i, end subscript, close vertical bar

The 
root mean square error (RMSE)
 can be calculated in several ways and it is equivalent to the 
sample standard deviation
 of the differences. The mean absolute error (MAE) is another commonly used metric in regression problems. A major advantage of RMSE and MAE is that the values are interpreted in the same units as the original data. MAE is the average of the absolute difference between the predicted values and observed value. Unlike RMSE all of the individual scores are weighted equally during the averaging. The squaring of the term in RMSE results in a higher penalty on larger differences when compared to MAE.

Classification
Most classification metrics start from a 
confusion matrix
.


Predicted False 
(
Y
^
=
0
)
( 
Y
^
 =0)left parenthesis, Y, with, hat, on top, equals, 0, right parenthesis

Predicted True 
(
Y
^
=
1
)
( 
Y
^
 =1)left parenthesis, Y, with, hat, on top, equals, 1, right parenthesis

True 
(
Y
=
0
)
(Y=0)left parenthesis, Y, equals, 0, right parenthesis

True Negatives 
(
T
N
)
(TN)left parenthesis, T, N, right parenthesis

False Positive 
(
F
P
)
(FP)left parenthesis, F, P, right parenthesis

True 
(
Y
=
1
)
(Y=1)left parenthesis, Y, equals, 1, right parenthesis

False Negatives 
(
F
N
)
(FN)left parenthesis, F, N, right parenthesis

True Positives 
(
T
P
)
(TP)left parenthesis, T, P, right parenthesis

  

 
accuracy
=
t
p
+
t
n
t
p
+
f
p
+
t
n
+
f
n
accuracy= 
tp+fp+tn+fn
tp+tn
​
 start text, a, c, c, u, r, a, c, y, end text, equals, start fraction, t, p, plus, t, n, divided by, t, p, plus, f, p, plus, t, n, plus, f, n, end fraction : overall proportion correct

precision
=
t
p
t
p
+
f
p
precision= 
tp+fp
tp
​
 start text, p, r, e, c, i, s, i, o, n, end text, equals, start fraction, t, p, divided by, t, p, plus, f, p, end fraction : proportion called true that are correct

recall
=
t
p
t
p
+
f
n
recall= 
tp+fn
tp
​
 start text, r, e, c, a, l, l, end text, equals, start fraction, t, p, divided by, t, p, plus, f, n, end fraction : proportion of true that are called correctly

The F1_score is the harmonic mean of precision and recall. There are different variants of the F1_score, notably the 
F
β
F 
β
​
 F, start subscript, beta, end subscript score.

F1_score=21recall+1precision

There are also several ways to average the F1_score when working in multi-class applications (e.g. weighted, micro, macro). The average parameter can change significantly the behavior and performance of your model especially when the classes are imbalanced.

Multi-class and multi-label metrics
Multiclass classification

The retention example from above is an example of multi-class classification because there are more than two classes. Multiclass classification works under the assumption that each sample is assigned to only a single label.

Multilabel classification

Multilabel classification uses a set of possible labels and allows multiple labels to be assigned to each sample. The labels are are not mutually exclusive.

Model Performance
How well a model performs can be decomposed as bias, variance and noise. Given a true model and infinite data we would be able to calibrate a model to reduce both bias and variance. These conditions are not realistic so there is a trade-off between the error that is related to model assumptions (bias) and the error that arises due to fluctuations in the training data (variance)

When a model exhibits high 
bias
Add more features such as engineered features or those derived from additional data

Use a more sophisticated model

Decrease regularization by tuning the regularization hyperparameter

When a model exhibits high 
variance
Use fewer features techniques like 
variance thresholding
, ANOVA (
SelectKBest
), manifold learning,and matrix decomposition can be used.

Use a simpler model because the model may be overly complex for a relatively simple signal in the data

Use more training samples

Increase regularization by tuning the regularization hyperparameter

Cross-validation along with train-test splits are critical tools to help ensure our models are well calibrated for unseen data. Cross-validation is often performed along with a grid-search over different configurations of hyperparameters. There are several variants of grid-searching including a non-exhaustive version that randomly searched. Model evaluation plots like the learning plot can help you understand the type of error associated with model performance and they can help you determine if you need more training data.

Linear models
All of these models make use of the following function:

y
^
(
x
,
w
)
=
w
0
+
w
1
x
1
,
+
…
+
w
p
x
p
,
y
^
​
 (x,w)=w 
0
​
 +w 
1
​
 x 
1
​
 ,+…+w 
p
​
 x 
p
​
 ,y, with, hat, on top, left parenthesis, x, comma, w, right parenthesis, equals, w, start subscript, 0, end subscript, plus, w, start subscript, 1, end subscript, x, start subscript, 1, end subscript, comma, plus, dots, plus, w, start subscript, p, end subscript, x, start subscript, p, end subscript, comma

The target y could be a column vector or a matrix in the multivariate case. There are 
p
pp features and 
p
pp coefficients. The intercept is written here as 
w
0
w 
0
​
 w, start subscript, 0, end subscript. If 
p
>
1
p>1p, is greater than, 1 then we are under the category of 
multiple linear regression
 a very common variant in the data science application space. The 
w
i
w 
i
​
 w, start subscript, i, end subscript:’s are parameters or weights.

Many of the linear models that are commonly used in data science like linear regression, the t-test and ANOVA are examples of the 
general linear model
. If we relax the assumption that residuals can only be normally distributed and we introduce the concept of a link function then we extend into 
generalized linear models
, of which logistic regression is the best example. One extension further from GLMs brings us into the family of models known as 
generalized linear mixed models (GLMM)
. GLMMs contain some of the most flexible and useful linear models available with the best example being 
multilevel models
.

With each extension comes the need for more sophisticated model inference methods. For example, GLMMs generally require Bayesian inference methods like 
MCMC
 sampling, while simple linear regression can be carried out with 
ordinary least squares
 approaches.

Gradient decent can also be used for inference for several methods including support vector machines and logistic regression. It is a powerful and flexible way to carry out inference on linear models and the results can compares favorably to even more sophisticated models.

Linear models also have a number of extensions including kernels and splines that enable non-linear functions. The level of model interpretation that is available with linear models and ease of implementation make them a safe choice for a baseline model. A baseline model is the one that you default to if a more sophisticated model cannot be shown to have superior performance.

TUTORIAL: Watson NLU
The 
Watson NLU service
 has the following features:

Categories: Categorize your content using a five-level classification hierarchy.

Concepts: Identify high-level concepts that aren’t necessarily directly referenced in the text.

Emotions: Analyze emotion conveyed by specific target phrases or by the document as a whole.

Entities: Find people, places, events, and other types of entities mentioned in your content.

Keywords: Search your content for relevant keywords.

Metadata: For HTML and URL input, get the author of the webpage, the page title, and the publication date.

Relations: Recognize when two entities are related, and identify the type of relation.

Semantic Role: Parse sentences into subject-action-object form, and identify entities and keywords that are subjects or objects of an action.

Sentiment: Analyze the sentiment toward specific target phrases and the sentiment of the document as a whole.

Custom Models: Identify custom entities and relations unique to your domain with Watson Knowledge Studio.

The service can use as input both URLs and text in the form of strings. Change the target_url below to see how the output changes. The service is used in a similar way to the other Watson services, like text classification and speech to text. Also it is good to keep in mind that Python is just one of several SDK APIs than can be used for NLU and other services.

CASE STUDY: connecting evaluation metrics and business metrics
Natural language processing, or NLP, is a subfield of linguistics, computer science, information engineering, and artificial intelligence. In the last ten years there has been an explosion of data, technology and deep-learning techniques that has advanced NLP applications.

Two prominent examples are speech recognition and conversational agents. Both of which are commonly used in business applications. With so many rapidly expanding areas of NLP, the ability to effectively use the available tools has become increasingly important. As you process the text within a corpus into a matrix that can be consumed by machine learning algorithms there are several questions to stop and ask yourself, because each domain has nuances that require the treatment of text to be customized. For example, a word or n-gram that appears in high frequency in one corpus may not be important, but in another it may hold importance. The phrase “we will” is probably of little importance in most corpora, but if we analyzed a corpus of political speeches it may be relevant.

Do I mask pronouns?

Which stop words do I include?

Which stemmer/lemmatizer is best?

Which n-grams do I include?

Do I filter based on frequency min an max?

TF or TFIDF or word embeddings?

There are non-frequency based representations of text that have proven useful in NLP with the 
word2vec
 group of models being among the most widely used. The model takes a corpus of text and produces a 
word embedding
, which is essentially a projection into a vector space that generally has a few hundred dimensions.

Additional resources

scikit-learn tutorial working with text


Building Machine Learning and Deep Learning Models
Tree-based methods
Tree-based methods represent an important middle ground between interpretable and black-box types of models. For some business questions like those that deal with marketing and sales data understanding ‘how’ a model makes a decision can contribute to a deployed model’s usefulness.

Decision trees are sometimes used by themselves as a classification or regression model. They are also an important base model for random forests and boosting, two commonly used ensemble models. Not long after the introduction of the bootstrap it was applied to to ensembles of decision trees, which is known as bootstrap aggregating or bagging. Although bagging is often discussed in the context of decision trees, it can be used with any base model.

If we consider a only a subset of features as we grow our decision trees during bootstrap aggregation then we arrive at the random forest model. The trees in boosting are correlated and random forests seeks to mitigate this by averaging slightly different, but fully grown trees. It does this through random subsetting of the features as the branches are grown in the tree. This in turn helps reduce the overall model variance.

The decision tree base models in random forests individually have low bias and high variance. When boosting is carried out with ensembles of trees they tent to be high bias and low variance. The individual trees are also grown sequentially focusing on the bias component of error. Adaboost is perhaps the most widely used boosting algorithm, but there are several others like XGBoost that tend to perform well. Boosting is often referred to as gradient boosting, because it uses gradient decent to carry out inference on the model.

Ensemble learning through model averaging, Bayesian model averaging, model stacking, majority vote and other methods are among the best performing methods known in machine learning. Large ensembles with sophisticated component models tend to have practical limitations like time to train and time to predict, but many of these limitations will be overcome as computers become more powerful, making this an important area to keep abreast of.

scikit-learn ensemble methods

TUTORIAL: Watson visual recognition
Some of the main features offered by the Watson visual recognition service are:

Train a custom model for visual inspection

Bring AI into iOS Apps with Core ML

Classify your images with no additional training

The tutorial used the Watson Visual Recognition service to classify images with built-in and a custom classifiers. The matplotlib library was used to display images. The classify capability can be used for local files or it can be pointed to a URL. As with the other services that we have seen the information is transmitted using JSON and there are multiple SDKs that enable access from different environments.

Watson Visual Recognition docs

Watson Visual Recognition overview

Watson Visual Recognition best practices

Neural networks
A 
multilayer perceptron (MLP)
 MLPs are sometimes referred to as vanilla neural networks. The number of hidden layers in a MLP and the size (number of notes in each) are configurable parameters that you will need to keep in mind when building neural networks.


MLP networks are comprised of an input layer, one or more hidden layers, and an output layer. Each neuron in the input layer corresponds to a feature and the information propagates from the input layer to the output layer. This is done using a weighted linear summation and a non-linear activation function. This left to right movement of information is known as feed-forward.

To train most neural networks an important algorithm called 
backpropagation
 is used. The algorithms is used to compute the gradient, while another algorithm, such as stochastic gradient descent, carries out the rest of learning process.

Notable types of artificial neural networks:

Multi-layer perceptron
Feed-forward neural network used for supervised learning. Often uses multiple layers and non-linear activation functions. Can be applied to most supervised learning tasks.

Autoencoders
It is a feed forward neural network that can be used to predict the feature matrix. Autoencoders may be thought of as being a special case of feedforward network used for unsupervised learning tasks. Example applications include dimension reduction and de-noising.

Convolutional neural network
A convolutional neural network (CNN, or ConvNet) uses image filters or kernels to learn patterns in a feature matrix. Commonly used to detect patterns in images and video.

Recurrent neural network
In RNNs connections between nodes can be cyclical, giving the network memory. Used for sequences: handwriting, speech recognition, time series. Once commonly used RNN architecture is the 
long short-term memory
.

CASE STUDY: Tensorflow
The case study demonstrated how to build, iterate on and compare several versions of a simple convolutional neural network (CNN). Among other use cases CNNs have been applied to:

image classification
,

image segmentation

image retrieval

Object detection

The 
Keras Sequential API
 was used during the case study to facilitate iteration. Some of the key features from the interface were:

model.summary() is a function is important for understanding the shape of tensors as they are passed between layers

models.Sequential() is an object that provides an intuitive and readable way to build the network

model.compile is needed before training the model

model.fit is used for model training and it works like a scikit-learn estimator

There are several options when it comes to 
saving and loading TensorFlow models
. Saved models can be deployed using with 
TFLite
, 
TensorFlow.js
, 
TensorFlow Serving
, or 
TensorFlow Hub
.