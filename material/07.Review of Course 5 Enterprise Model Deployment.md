Review of Course 5: Enterprise Model Deployment
Deploying Models
Data at scale
When it comes to preparing a model for deployment there is a guiding set of steps that may be useful.

Make it work

Make it better

Then make it faster

Before moving into a 
high-performance computing (HPC)
 environment like Apache Spark there are some optimizations that might improve performance of models once deployed. In some cases the optimizations are enough to avoid the HPC environment entirely. Some of the important Python packages for code optimization are:

Multiprocessing
 - This is a package in the standard Python library and it supports spawning processes (for each core) using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency

Threading
 - Another package in the standard library that allows separate flows flow of execution at a lower level than multiprocessing.

Subprocessing
 - Module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. You may run and control non-Python processes like Bash or R with the subprocessing module.

mpi4py
 - MPI for Python provides bindings of the Message Passing Interface (MPI) standard for the Python programming language, allowing any Python program to exploit multiple processors.

ipyparallel
 - Parallel computing tools for use with Jupyter notebooks and IPython. Can be used with mpi4py.

Cython
 - An optimizing static compiler for both the Python programming language and the extended Cython programming language It is generally used to write C extensions for slow portions of code.

PyCUDA
. - Python package that allows parallel computing on GPUs via 
CUDA (Compute Unified Device Architecture)

Supercomputers and parallel computing can help with model training, prediction and other related tasks, but it is worth noting that there are two laws that constrain the maximum speed-up of computing: 
Amdahl’s law
 and 
Gustafson’s law
.

Dealing with data at scale, which is closely related to both code optimization and parallel computing. 
Apache Spark
, is an example of a 
cluster computing
 framework that enables to enable parallel computing.

If we talk about scale in the context of a program or model we may be referring to any of the following questions. Let the word service in this context be both the deployed model and the infrastructure.

Does my service train in a reasonable amount of time given a lot more data?

Does my service predict in a reasonable amount of time given a lot more data?

Is my service ready to support additional request load?

Docker and containers
Technologies that are commonly used in an environment with numerous Docker containers.

Docker Compose
 - Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services.

Kubernetes
 - An open-source system for automating deployment, scaling, and management of containerized applications.

Jenkins
 - Tool to help automate the model deployment process. Often used for Continuous Integration/Continuous Delivery (CI/CD).

Ansible
 - A tool for automation to the provision of the target environment and to then deploy the application

Watson Machine Learning Tutorial
Watson Machine Learning (WML)
 is an IBM service that makes deploying a model for prediction and/or training relatively easy. The 
Watson Machine Learning Python client
 was used in this tutorial to connect to the service. You may train, test and deploy your models as APIs for application development, then share the models with colleagues. In this tutorial you saw how you could match your local environment to the requirements of the available runtime environments in WML. You also have the option of iterating on models in Watson Studio and then using nearly the same code those same models could be deployed.

Deploying Models using Spark
Spark Machine Learning
There are two APIs for Spark MLlib. The RDD-based API and the dataframe based API, which is often referred to as Spark ML. Each has its own documentation.

Spark MLlib docs

Spark ML docs

Spark MLlib has a suite of available tools for unsupervised learning—namely dimension reduction and clustering. For clustering K-means and Gaussian Mixture Models (GMMs) are the main tools. Latent Dirichlet Allocation (LDA) is available as a tool for clustering over documents of natural language.

Spark clustering documentation

Spark MLlib has a number of available supervised learning algorithms that is classification and regression. Many of the commonly used algorithms have been implemented including: random forests, gradient boosted trees, linear support vector machines and even basic multilayer perceptrons.

Spark MLlib - supervised learning

Spark Recommenders
The majority of modern 
recommender systems
 embrace either a collaborative filtering or a content-based approach. A number of other approaches and hybrids exists making some implemented systems difficult to categorize.

Collaborative filtering
 - Based off of a ratings matrix collaborative filtering is a family of methods that infers a subset of users that have behavior similar to a particular user. The items preferred by these users are combined and filtered to create a ranked list of recommended items.

Content-based filtering
 - Predictions are made based on the properties/characteristics of an item. User behavior is not considered.

Matrix factorization
 is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of lower dimensionality matrices.

There are several Python packages available to help create recommenders including 
surprise
. Because scale with respect to prediction is often a concern for recommender systems many production environments use the implementation found in Spark MLlib. The 
Spark collaborative filtering
 implementation uses 
Alternating least Squares
.

CASE STUDY: model deployment
We used a Docker image to create a local Spark environment. In this environment a recommender systems was created using Spark MLlib’s collaborative filtering implementation. The model itself was tuned, by modifying hyperparameters and by toggling between implicit and explicit versions of the underlying algorithm. spark-submit was used to simulate a deployed model.